{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py37/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File ID</th>\n",
       "      <th>Assessment</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Subjective Sections</th>\n",
       "      <th>Objective Sections</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188026.txt</td>\n",
       "      <td>H/O HYPERKALEMIA (HIGH POTASSIUM, HYPERPOTASSE...</td>\n",
       "      <td># Hypoxia:; Hyperkalemia</td>\n",
       "      <td>ULTRASOUND - At [**2121-3-16**] 11:32 AM\\n- MD...</td>\n",
       "      <td>Last dose of Antibiotics:\\nAzithromycin - [**2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101616.txt</td>\n",
       "      <td>67 y/o M CAD s/[**Initials (NamePattern4) **] ...</td>\n",
       "      <td>Lower GI bleed; Hypotension; CAD</td>\n",
       "      <td>COLONOSCOPY - At [**2171-1-25**] 12:31 PM\\n- C...</td>\n",
       "      <td>Last dose of Antibiotics:\\nInfusions:\\nOther I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102486.txt</td>\n",
       "      <td>81F with h/o chronic eosinophilic lung disease...</td>\n",
       "      <td>PULMONARY EMBOLISM; FEVER; HYPOTENSION; OLIGUR...</td>\n",
       "      <td>Pleuritic right chest pain\\n- patient started ...</td>\n",
       "      <td>Last dose of Antibiotics:\\nCiprofloxacin - [**...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>198989.txt</td>\n",
       "      <td>79 yo F w/ a h/o CHF (EF of 20-30%), carotid s...</td>\n",
       "      <td>Sepsis; Altered/Depressed MS\\n thought to be [...</td>\n",
       "      <td>- ID: rec bedside echo\\n- continued fluid bolu...</td>\n",
       "      <td>Last dose of Antibiotics:\\nMetronidazole - [**...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>193604.txt</td>\n",
       "      <td>Mr. [**Known firstname 1908**] [**Known lastna...</td>\n",
       "      <td># Bradycardia / Rhythm; #. Hypertension; # CAD...</td>\n",
       "      <td>High-grade AV nodal block\\n- Had successful [*...</td>\n",
       "      <td>Last dose of Antibiotics:\\nInfusions:\\nOther I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      File ID                                         Assessment  \\\n",
       "0  188026.txt  H/O HYPERKALEMIA (HIGH POTASSIUM, HYPERPOTASSE...   \n",
       "1  101616.txt  67 y/o M CAD s/[**Initials (NamePattern4) **] ...   \n",
       "2  102486.txt  81F with h/o chronic eosinophilic lung disease...   \n",
       "3  198989.txt  79 yo F w/ a h/o CHF (EF of 20-30%), carotid s...   \n",
       "4  193604.txt  Mr. [**Known firstname 1908**] [**Known lastna...   \n",
       "\n",
       "                                             Summary  \\\n",
       "0                           # Hypoxia:; Hyperkalemia   \n",
       "1                   Lower GI bleed; Hypotension; CAD   \n",
       "2  PULMONARY EMBOLISM; FEVER; HYPOTENSION; OLIGUR...   \n",
       "3  Sepsis; Altered/Depressed MS\\n thought to be [...   \n",
       "4  # Bradycardia / Rhythm; #. Hypertension; # CAD...   \n",
       "\n",
       "                                 Subjective Sections  \\\n",
       "0  ULTRASOUND - At [**2121-3-16**] 11:32 AM\\n- MD...   \n",
       "1  COLONOSCOPY - At [**2171-1-25**] 12:31 PM\\n- C...   \n",
       "2  Pleuritic right chest pain\\n- patient started ...   \n",
       "3  - ID: rec bedside echo\\n- continued fluid bolu...   \n",
       "4  High-grade AV nodal block\\n- Had successful [*...   \n",
       "\n",
       "                                  Objective Sections  \n",
       "0  Last dose of Antibiotics:\\nAzithromycin - [**2...  \n",
       "1  Last dose of Antibiotics:\\nInfusions:\\nOther I...  \n",
       "2  Last dose of Antibiotics:\\nCiprofloxacin - [**...  \n",
       "3  Last dose of Antibiotics:\\nMetronidazole - [**...  \n",
       "4  Last dose of Antibiotics:\\nInfusions:\\nOther I...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset, load_metric, Dataset, DatasetDict\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,3'\n",
    "data_dir=\"data\"\n",
    "path = data_dir+\"/bionlp-workshop-2023-shared-task-1a-problem-list-summarization-1.0.0/BioNLP2023-1A-Train.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ID\n",
      "188026.txt\n",
      "====================================\n",
      "Assessment\n",
      "H/O HYPERKALEMIA (HIGH POTASSIUM, HYPERPOTASSEMIA)\n",
      " .H/O HYPERGLYCEMIA\n",
      " CHRONIC OBSTRUCTIVE PULMONARY DISEASE (COPD, BRONCHITIS, EMPHYSEMA)\n",
      "   WITH ACUTE EXACERBATION\n",
      "   A 59 year-old man presents with malaise and hypoxia\n",
      "====================================\n",
      "Summary\n",
      "# Hypoxia:; Hyperkalemia\n",
      "====================================\n",
      "Subjective Sections\n",
      "ULTRASOUND - At [**2121-3-16**] 11:32 AM\n",
      "- MD [**First Name (Titles) 2745**] [**Last Name (Titles) 11871**] [**Name (NI) 1270**] but coulnd't leave voicemail because her\n",
      "voicemail wasn't set up\n",
      "- patient refused NGT for kayexalate; K 6.5 at noon; ECG unremarkable.\n",
      "Was able to take PO well in p.m., received kayexalate; K down to 5.0\n",
      "- renal u/s: no hydronephrosis\n",
      "No Known Drug Allergies\n",
      "Changes to  and\n",
      "f\n",
      "Review of systems is unchanged from admission except as noted below\n",
      "Review of systems:\n",
      "====================================\n",
      "Objective Sections\n",
      "Last dose of Antibiotics:\n",
      "Azithromycin - [**2121-3-16**] 10:46 AM\n",
      "Infusions:\n",
      "Other ICU medications:\n",
      "Furosemide (Lasix) - [**2121-3-16**] 02:45 PM\n",
      "Heparin Sodium (Prophylaxis) - [**2121-3-16**] 04:23 PM\n",
      "Other medications:\n",
      "Flowsheet Data as of  [**2121-3-17**] 08:14 AM\n",
      "Vital signs\n",
      "Hemodynamic monitoring\n",
      "Fluid balance\n",
      "24 hours\n",
      "Since [**23**] AM\n",
      "Tmax: 36.7\n",
      "C (98\n",
      "Tcurrent: 36.6\n",
      "C (97.8\n",
      "HR: 72 (54 - 72) bpm\n",
      "BP: 120/59(74) {90/49(60) - 162/128(136)} mmHg\n",
      "RR: 20 (12 - 25) insp/min\n",
      "SpO2: 100%\n",
      "Heart rhythm: SR (Sinus Rhythm)\n",
      "Wgt (current): 123.3 kg (admission): 122.7 kg\n",
      "Total In:\n",
      "2,220 mL\n",
      "PO:\n",
      "970 mL\n",
      "TF:\n",
      "IVF:\n",
      "1,250 mL\n",
      "Blood products:\n",
      "Total out:\n",
      "1,137 mL\n",
      "290 mL\n",
      "Urine:\n",
      "1,137 mL\n",
      "290 mL\n",
      "NG:\n",
      "Stool:\n",
      "Drains:\n",
      "Balance:\n",
      "1,083 mL\n",
      "-290 mL\n",
      "Respiratory support\n",
      "O2 Delivery Device: CPAP mask\n",
      "Ventilator mode: CPAP/PSV\n",
      "Vt (Spontaneous): 340 (340 - 340) mL\n",
      "PS : 10 cmH2O\n",
      "RR (Spontaneous): 18\n",
      "PEEP: 5 cmH2O\n",
      "FiO2: 50%\n",
      "PIP: 15 cmH2O\n",
      "SpO2: 100%\n",
      "ABG: 7.29/61/84.[**Numeric Identifier 1625**]/30/0\n",
      "Ve: 6.4 L/min\n",
      "PaO2 / FiO2: 168\n",
      "Peripheral Vascular: (Right radial pulse: Not assessed), (Left radial\n",
      "pulse: Not assessed), (Right DP pulse: Not assessed), (Left DP pulse:\n",
      "Not assessed)\n",
      "Skin:  Not assessed\n",
      "Neurologic: Responds to: Not assessed, Movement: Not assessed, Tone:\n",
      "Not assessed\n",
      "/\n",
      "254 K/uL\n",
      "12.0 g/dL\n",
      "221 mg/dL\n",
      "2.1 mg/dL\n",
      "30 mEq/L\n",
      "5.0 mEq/L\n",
      "37 mg/dL\n",
      "102 mEq/L\n",
      "138 mEq/L\n",
      "37.6 %\n",
      "12.6 K/uL\n",
      "image002.jpg]\n",
      "[**2121-3-16**]  01:50 AM\n",
      "[**2121-3-16**]  05:25 AM\n",
      "[**2121-3-16**]  07:42 AM\n",
      "[**2121-3-16**]  09:06 AM\n",
      "[**2121-3-16**]  06:10 PM\n",
      "[**2121-3-17**]  05:14 AM\n",
      "WBC\n",
      "12.6\n",
      "12.6\n",
      "Hct\n",
      "40.2\n",
      "37.6\n",
      "Plt\n",
      "251\n",
      "254\n",
      "Cr\n",
      "2.5\n",
      "2.5\n",
      "2.1\n",
      "TCO2\n",
      "30\n",
      "33\n",
      "31\n",
      "Glucose\n",
      "[**Telephone/Fax (3) 11872**]\n",
      "Other labs: CK / CKMB / Troponin-T:44//, Differential-Neuts:91.4 %,\n",
      "Lymph:5.7 %, Mono:1.9 %, Eos:0.7 %, Lactic Acid:1.3 mmol/L, Ca++:7.9\n",
      "mg/dL, Mg++:1.8 mg/dL, PO4:4.4 mg/dL\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "for col_name in df.columns:\n",
    "    print(col_name)\n",
    "    print(df.iloc[0][col_name])\n",
    "    print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source_text'] = \"summarize: \" + \" <ASSESSMENT> \"+ df['Assessment'] + \" <SUBJECTIVE> \"+ df['Subjective Sections'] +\" <OBJECTIVE> \" + df['Objective Sections']\n",
    "df['target_text'] = df[\"Summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((612, 7), (153, 7))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.applymap(str)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=2023)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "my_dataset_dict = DatasetDict({\"train\":train_dataset,\"test\":test_dataset,'validation':test_dataset})\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [\"hello there\", \"general kenobi\"]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_checkpoint = \"t5-base\"\n",
    "# model_checkpoint = \"t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "#     prefix = \"summarize: \"\n",
    "# else:\n",
    "#     prefix = \"\"\n",
    "prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"source_text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"target_text\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[21603, 10, 3, 2, 3291, 134, 10087, 11810, 3155, 6897, 3, 63, 32, 377, 6, 3150, 1695, 113, 6621, 28, 12085, 11, 5738, 1406, 6, 10246, 12, 8, 27, 5211, 21, 10950, 13177, 5, 3, 2, 4138, 279, 683, 14196, 8087, 3155, 272, 5017, 7039, 205, 4254, 25380, 308, 3, 18, 486, 784, 19844, 2658, 2079, 3486, 18, 2555, 19844, 908, 11484, 10, 4118, 3246, 3, 5905, 9730, 205, 4254, 25380, 3, 18, 486, 784, 19844, 2658, 2079, 3486, 18, 2555, 19844, 908, 11484, 10, 4118, 3246, 15971, 434, 12222, 5359, 3, 18, 486, 784, 19844, 2658, 2079, 3486, 18, 2555, 19844, 908, 11484, 10, 4118, 3246, 262, 18256, 3, 18, 486, 784, 19844, 2658, 2079, 3486, 18, 2555, 19844, 908, 14146, 10, 1206, 3246, 784, 19844, 2658, 2079, 3486, 18, 2555, 19844, 908, 10, 17656, 4227, 3, 12380, 19, 4863, 117, 10118, 262, 18256, 24, 3217, 22726, 12110, 1707, 13, 584, 8525, 2904, 26164, 5, 465, 984, 8521, 5, 3, 195, 49, 4044, 7, 10, 4511, 31027, 7, 8475, 5398, 180, 2091, 53, 13456, 17, 5397, 41, 7395, 138, 61, 41, 134, 83, 89, 265, 15, 189, 32, 226, 17694, 109, 87, 19310, 3493, 10776, 5397, 61, 9053, 107, 117, 5968, 7, 12, 11, 3, 89, 4543, 13, 1002, 19, 26164, 45, 7209, 3578, 38, 4466, 666, 4543, 13, 1002, 10, 3, 2, 10539, 683, 14196, 8087, 3155, 2506, 6742, 13, 4066, 18317, 7, 10, 86, 7316, 7, 10, 2502, 27, 5211, 11208, 10, 4574, 22230, 15, 180, 83, 89, 342, 3, 18, 784, 19844, 2658, 2079, 3486, 18, 2555, 19844, 908, 12811, 10, 1808, 3246, 216, 1893, 77, 3, 28637, 41, 345, 29006, 40, 8606, 7, 61, 3, 18, 784, 19844, 2658, 2079, 3486, 18, 2555, 19844, 908, 3, 3076, 10, 2128, 3246, 2502, 11208, 10, 3, 15390, 12230, 2747, 38, 13, 784, 19844, 2658, 2079, 3486, 18, 2577, 19844, 908, 10668, 10, 2596, 5422, 23736, 3957, 216, 51, 32, 16928, 4891, 30990, 2109, 997, 716, 1541, 784, 19844, 4959, 19844, 908, 5422, 332, 9128, 10, 6654, 205, 11704, 22776, 332, 14907, 10, 220, 25791, 205, 41, 3264, 8383, 10, 3, 4613, 41, 4013, 3, 18, 13736, 61, 3, 115, 2028, 3, 11165, 10, 3, 3916, 87, 3328, 599, 3951, 61, 3, 2, 5865, 87, 3747, 599, 4177, 61, 3, 18, 3, 18959, 87, 2518, 599, 3940, 61, 2, 3, 635, 566, 122, 3, 12224, 10, 627, 16465, 3, 18, 1401, 61, 16, 7, 102, 87, 1109, 2526, 667, 357, 10, 3, 21962, 6219, 10638, 10, 3, 6857, 41, 134, 77, 302, 3, 27224, 61, 9273, 86, 10, 1914, 2915, 3, 51, 434, 3, 11944, 3, 51, 434, 9915, 10, 7366, 3, 51, 434, 3, 11944, 3, 51, 434, 3, 9164, 10, 27, 22502, 10, 2899, 3, 51, 434, 12737, 494, 10, 9273, 91, 10, 668, 2518, 3, 51, 434, 3, 17147, 3, 51, 434, 4575, 630, 10, 668, 2518, 3, 51, 434, 3, 17147, 3, 51, 434, 3, 12531, 10, 180, 13650, 10, 15730, 7, 10, 17904, 10, 12778, 3, 51, 434, 3, 18, 19947, 3, 51, 434, 1], [21603, 10, 3, 2, 3291, 134, 10087, 11810, 3155, 14011, 188, 397, 147, 2777, 220, 2773, 19844, 908, 3, 63, 87, 32, 283, 28, 892, 13, 9302, 371, 6, 3, 58, 25032, 308, 113, 47, 1622, 12, 8, 3, 2326, 45, 784, 19844, 4489, 7675, 1947, 586, 19844, 908, 419, 6111, 21, 19944, 19285, 5, 3, 2, 4138, 279, 683, 14196, 8087, 3155, 3, 18, 3, 102, 17, 31, 7, 19944, 2637, 410, 59, 1172, 28, 13592, 13, 112, 2106, 345, 2965, 6, 5510, 3, 5490, 16, 16, 14535, 257, 87, 31564, 18426, 4511, 31027, 7, 34, 8509, 117, 312, 1621, 89, 24938, 4268, 29, 9053, 107, 117, 3657, 35, 17694, 20455, 30095, 597, 5661, 117, 5968, 7, 12, 4543, 13, 1002, 19, 26164, 45, 7209, 3578, 38, 4466, 666, 4543, 13, 1002, 10, 3, 2, 10539, 683, 14196, 8087, 3155, 2506, 6742, 13, 4066, 18317, 7, 10, 86, 7316, 7, 10, 531, 102, 8721, 3, 18, 431, 3, 51, 75, 122, 87, 439, 122, 87, 1109, 2502, 27, 5211, 11208, 10, 6343, 8115, 6983, 15, 41, 3612, 7, 2407, 61, 3, 18, 784, 19844, 357, 2517, 16737, 6832, 19844, 908, 25095, 5422, 6650, 9, 12423, 265, 41, 5000, 3843, 61, 3, 18, 784, 19844, 357, 2517, 16737, 6832, 19844, 908, 3, 632, 18828, 5422, 377, 295, 6820, 40, 3, 18, 784, 19844, 357, 2517, 16737, 6832, 19844, 908, 13574, 10, 4560, 5422, 2502, 11208, 10, 3, 15390, 12230, 2747, 38, 13, 784, 19844, 357, 2517, 16737, 6832, 19844, 908, 13574, 10, 2266, 5422, 23736, 3957, 216, 51, 32, 16928, 4891, 30990, 2109, 997, 716, 1541, 784, 19844, 4552, 19844, 908, 5422, 332, 9128, 10, 220, 25791, 205, 41, 3916, 5, 1298, 332, 14907, 10, 220, 25791, 205, 41, 3916, 5, 1298, 8383, 10, 3, 3914, 41, 4433, 3, 18, 335, 12703, 3, 115, 2028, 3, 11165, 10, 3, 12869, 87, 3449, 599, 3072, 61, 3, 2, 4389, 87, 4177, 599, 5062, 61, 3, 18, 5864, 87, 3328, 599, 4440, 61, 2, 3, 635, 566, 122, 3, 12224, 10, 627, 17251, 3, 18, 5400, 61, 16, 7, 102, 87, 1109, 2526, 667, 357, 10, 668, 6370, 6219, 10638, 10, 5097, 41, 134, 77, 302, 2067, 11971, 16464, 9, 61, 9273, 86, 10, 209, 3, 51, 434, 314, 4867, 3, 51, 434, 9915, 10, 3, 9164, 10, 27, 22502, 10, 209, 3, 51, 434, 314, 4867, 3, 51, 434, 12737, 494, 10, 9273, 91, 10, 3, 18365, 3, 51, 434, 3147, 3, 51, 434, 4575, 630, 10, 3, 18365, 3, 51, 434, 3147, 3, 51, 434, 3, 12531, 10, 180, 13650, 10, 15730, 7, 10, 17904, 10, 3, 4949, 3390, 3, 51, 434, 3, 27025, 3, 51, 434, 7127, 2388, 6546, 380, 411, 357, 15715, 15511, 10, 3720, 32, 6471, 88, 138, 8017, 13713, 173, 1016, 2175, 10, 205, 12569, 87, 3291, 134, 13582, 87, 16204, 15390, 584, 17, 41, 17175, 61, 10, 2899, 41, 2560, 3, 18, 2899, 61, 3, 51, 434, 3, 12224, 41, 17175, 61, 10, 898, 276, 28367, 10, 505, 2446, 566, 357, 667, 3188, 667, 357, 10, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[29378, 13177, 10, 1], [1713, 7127, 2388, 6546, 2678, 9746, 117, 9302, 371, 6, 117, 3, 58, 25032, 308, 1]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(my_dataset_dict['train'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.44ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.04ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.36ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = my_dataset_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16//len(os.environ['CUDA_VISIBLE_DEVICES'].split(\",\"))\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"models/{model_name}-finetuned-xsum\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=20,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Objective Sections, target_text, source_text, Summary, Assessment, __index_level_0__, File ID, Subjective Sections.\n",
      "/opt/conda/envs/py37/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 612\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 780\n",
      "/opt/conda/envs/py37/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 53/780 01:03 < 15:06, 0.80 it/s, Epoch 1.33/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.966501</td>\n",
       "      <td>15.178700</td>\n",
       "      <td>3.994000</td>\n",
       "      <td>13.430300</td>\n",
       "      <td>13.431900</td>\n",
       "      <td>18.150300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: Objective Sections, target_text, source_text, Summary, Assessment, __index_level_0__, File ID, Subjective Sections.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 153\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3137008/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/py37/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1368\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m                     \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m                 ):\n\u001b[1;32m   1372\u001b[0m                     \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "420fd8acafbdecb40a89ccb9e2075ab2270a58b481060b384091ca9af271973a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
